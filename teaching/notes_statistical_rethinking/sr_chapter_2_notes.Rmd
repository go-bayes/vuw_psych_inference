---
title: "sr_chapter_2_notes"
author: "Joseph Bulbulia"
date: "01/12/2020"
output: html_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Highlights from Chapter 2 in Statistical Rethinking

A few key points in Statistical Rethinking Chapter 2

## Small worlds and large worlds

* We want to infer how the world is -- the large world, however observations come from some part of that larger world
* How to build a model: in the most basic sense, we want to consider ways the world might be that are consistent with our observations. (The garden of forking data)

## Building a model for the globe toss example. 


First some notation:


Some notation: 

This says W is distributed N (= W + L ways)  with an unknown probability p per trial that is sampled from a binomial distribution:

$W \sim Binomial(N,p)$

and we assume (here) that p is sampling from a uniform distribution (aka has a uniform range) from zero to 1, which we write as

$p \sim Uniform(0,1)$



This is how RM creates some data. Before we get started here's a tip about how to drop scientific notation in R: `r options(scipen=999)`


```{r}
options(scipen=999)
# first play around with the dbinom function in R
```

```{r}
dbinom( 6 , size=9, prob = .5 ) # what is the probability of the relative number of ways to get six water, holding p at 0.5 and n = w + l  at 9. 

# define grid: this is a vector of 20 points from 0 and 1
p_grid <- seq( from=0 , to=1 , length.out=20 )
p_grid  # check it

# define prior
prior <- rep( 1 , 20 )

prior  # check it: 20 x 1 in a vector

# compute likelihood at each value in grid # this is how R generates random numbers. Here are random numbers 
likelihood <- dbinom( 6 , size=9 , prob=p_grid )  # from the chapter, there were six water draws in nine tosses of the globe.  The likelihood is the probability of evidence given the hypothesis.  Here, we're saying that pr could be anything from 0 to 1 with equal probability.

likelihood

# compute product of likelihood and prior
unstd.posterior <- likelihood * prior  # we're taking the likelihood at face value.
unstd.posterior

# standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)
posterior
```

We can plot this object:


```{r}
plot( p_grid , posterior , type= "b",
       xlab= "probability of water", 
       ylab="posterior probability")
```


Not in the book. Consider what happens if we adjust our priors:

```{r}
# Here we take any value under five and we assign it a zero for our prior. We know that the world is at least half water

prior <- ifelse( p_grid < 0.5 , 0 , 1)

prior <- exp( -5*abs( p_grid - 0.5 ) ) 

# Here "abs is absolute value" and constraining the values to be lower than 1, bust softer than an absolute cutoff at .5

# next replot
unstd.posterior <- likelihood * prior  # we're taking the likelihood at face value.
unstd.posterior

# standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)

```

```{r}
plot( p_grid , posterior , type="b",
       xlab= "probability of water", 
       ylab="new posterior probability")
```



# Our first model using the rethinking package

```{r}
# load the rethinking package
library(rethinking)

# This is a model specified in the rethinking syntax
globe.qa <- quap(
  alist(
    W ~ dbinom( W+L,p) ,# binomial likelihood
    p ~ dunif(0,1)# uniform prior
  ),
  data=list(W=6,L=3) )
# display summary of quadratic approximation
precis( globe.qa )
```

Note: "The quadratic approximation, either with a uniform prior or with a lot of data, is often equivalent to a MAXIMUM LIKELIHOOD ESTIMATE (MLE) and its STANDARD ERROR."

```{r}
m2 <- quap(
  alist(
    W ~ dbinom( W+L,p) ,# binomial likelihood
    p ~ dnorm(.5,.1)# normal distribution with mean .5 and an sd of .1
  ),
  data=list(W=6,L=3) )
# display summary of quadratic approximation ## 
precis( m2 )
```


# Practicalities (not in the book)

Solomon Kurz's code for replicating the rethinking plots:
[https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/small-worlds-and-large-worlds.html](https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/small-worlds-and-large-worlds.html)


Let's use his code to run a bayesian model in brms: 


```{r echo =FALSE, cache =TRUE}
# install if you don't have this
library("brms")
# I use these options
rstan_options(auto_write=TRUE)
options(mc.cores=parallel::detectCores ()) # this runs all the cores on your machine

# Here there are more 4 x more data points

w = 24
n = 36

m3 <-
  brm(data = list(w = w, n = n), 
      family = binomial(link = "identity"),
      w|trials(n)  ~ 1,
      prior(beta(1, 1), class = Intercept),
      iter = 4000, warmup = 1000,
      control = list(adapt_delta = .9),
      seed = 4)
summary(m3)

```


```{r}
library("parameters")
parameters::model_parameters(m3)

```

Now some plots
```{r}
library("performance")
plot(parameters::model_parameters(m3))
```

Here's a some code from the Kurz book.  Here's how you can create a "tibble" or a fancy dataframe. 


```{r}
# load the tidyverse library
library(tidyverse)

# create a tibble
d <-
  tibble(p_1 = 0,
         p_2 = rep(1:0, times = c(1, 3)),
         p_3 = rep(1:0, times = c(2, 2)),
         p_4 = rep(1:0, times = c(3, 1)),
         p_5 = 1)
head(d)
```

Make a plot

```{r}
d<- d %>% 
  gather() %>%  # this takes a wide dataframe and makes it long
  mutate(x = rep(1:4, times = 5),
         possibility = rep(1:5, each = 4)) # this creates a new possibility column


d %>% 
  ggplot(aes(x = x, y = possibility, 
             fill = value %>% as.character())) +
  geom_point(shape = 21, size = 5) +
  scale_fill_manual(values = c("white", "navy")) +
  scale_x_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(.75, 4.25),
                  ylim = c(.75, 5.25)) +
  theme(legend.position = "none")
```

You can follow the rest of Solomon's work for creating graphs of the kind in SR. 


You can do the rest of the graph at your own leasure. I just wanted to show you how to make a "tibble." Later on, methods like this will be useful for creating simulated datasets. 







