---
title: "sr_chapter_2_notes"
author: "Joseph Bulbulia"
date: "01/12/2020"
output: html_document
---
  
  ```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Highlights from Chapter 2 in Statistical Rethinking

A few key points in Statistical Rethinking Chapter 2

## Small worlds and large worlds

* We want to infer how the world is -- the large world, however observations come from some part of that larger world
* How to build a model: in the most basic sense, we want to consider ways the world might be that are consistent with our observations. (The garden of forking data)

## Building a model for the globe toss example. 


First some notation:


Some notation: 

This says W is distributed N (= W + L ways)  with an unknown probability p per trial that is sampled from a binomial distribution:

$W \sim Binomial(N,p)$

and we assume (here) that p is sampling from a uniform distribution (aka has a uniform range) from zero to 1, which we write as

$p \sim Uniform(0,1)$



This is how RM creates some data. Before we get started here's a tip about how to drop scientific notation in R: `r options(scipen=999)`


```{r}
options(scipen=999)
# first play around with the dbinom function in R

dbinom( 6 , size=9, prob = .5 ) # what is the probability of the relative number of ways to get six water, holding p at 0.5 and n = w + l  at 9. 


# define grid: this is a vector of 20 points from 0 and 1
p_grid <- seq( from=0 , to=1 , length.out=20 )

p_grid

# define prior
prior <- rep( 1 , 20 )

prior  # 20 1's in a row

# compute likelihood at each value in grid # this is how R generates random numbers. Here are random numbers 
likelihood <- dbinom( 6 , size=9 , prob=p_grid )  # from the chapter, there were six water draws in nine tosses of the globe.  The likelihood is the probability of evidence given the hypothesis.   #Here, we're saying that pr could be anything from 0 to 1 with equal probability.

likelihood

# compute product of likelihood and prior
unstd.posterior <- likelihood * prior  # we're taking the likelihood at face value.

unstd.posterior

# standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)
posterior
```

We can plot this object:


```{r}
plot( p_grid , posterior , type="b",
       xlab= "probability of water", 
       ylab="posterior probability")
```




We are now in the position to change this plot by adjusting our priors:

```{r}
# here we take any value under five and we assign it a zero for our prior. We know that the world is at least half water
prior <- ifelse( p_grid < 0.5 , 0 , 1)

prior <- exp( -5*abs( p_grid - 0.5 ) ) # Here "abs is absolute value"  and constraining the values to be lower than 1, bust softer than an absolute cutoff at .5

# next replot
unstd.posterior <- likelihood * prior  # we're taking the likelihood at face value.
unstd.posterior

# standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)

```

```{r}
plot( p_grid , posterior , type="b",
       xlab= "probability of water", 
       ylab="new posterior probability")
```

# Questions
Explain RM's meaning here:

> It is common to hear that there is a minimum number of observations for a useful statistical estimate. For example, there is a widespread superstition that 30 observations are needed before one can use a Gaussian distribution. Why? In non-Bayesian statistical inference, procedures are often justified by the method’s behavior at very large sample sizes, so-called asymptotic behavior. As a result, performance at small samples sizes is questionable.
In contrast, Bayesian estimates are valid for any sample size. This does not mean that more data isn’t helpful—it certainly is. Rather, the estimates have a clear and valid interpretation, no matter the sample size. But the price for this power is dependency upon the initial plausibilities, the prior. If the prior is a bad one, then the resulting inference will be misleading.




# Our first model using the rethinking package

```{r}
# load the rethinking package
library(rethinking)

# This is a model specified in the rethinking syntax
globe.qa <- quap(
  alist(
    W ~ dbinom( W+L,p) ,# binomial likelihood
    p ~ dunif(0,1)# uniform prior
  ),
  data=list(W=6,L=3) )
# display summary of quadratic approximation
precis( globe.qa )
```

Note: "The quadratic approximation, either with a uniform prior or with a lot of data, is often equivalent to a MAXIMUM LIKELIHOOD ESTIMATE (MLE) and its STANDARD ERROR."

```{r}
m2 <- quap(
  alist(
    W ~ dbinom( W+L,p) ,# binomial likelihood
    p ~ dnorm(.5,.1)# normal distribution with mean .5 and an sd of .1
  ),
  data=list(W=6,L=3) )
# display summary of quadratic approximation ## 
precis( m2 )
```


# Practicalities (not in the book)

Solomon Kurz's code for replicating the rethinking plots:
[https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/small-worlds-and-large-worlds.html](https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/small-worlds-and-large-worlds.html)


Let's use his code to run a bayesian model in brms: 


```{r echo =FALSE}
# install if you don't have this
library(brms)
# I use these options
rstan_options(auto_write=TRUE)
options(mc.cores=parallel::detectCores ()) # this runs all the cores on your machine

# Here there are more 4 x more data points

w = 24
n = 36

m3 <-
  brm(data = list(w = w, n = n), 
      family = binomial(link = "identity"),
      w|trials(n)  ~ 1,
      prior(beta(1, 1), class = Intercept),
      iter = 4000, warmup = 1000,
      control = list(adapt_delta = .9),
      seed = 4)
summary(m3)

```


```{r}
library("parameters")
parameters::model_parameters(m3)

```

Now some plots
```{r}
library("performance")
parameters::model_parameters(m3)
```

Here's a some code from the Kurz boook


```{r}
# load the tidyverse library
library(tidyverse)

# create a tibble
d <-
  tibble(p_1 = 0,
         p_2 = rep(1:0, times = c(1, 3)),
         p_3 = rep(1:0, times = c(2, 2)),
         p_4 = rep(1:0, times = c(3, 1)),
         p_5 = 1)

head(d)
```

Make a plot

```{r}
d<- d %>% 
  gather() %>%  # this takes a wide dataframe and makes it long
  mutate(x = rep(1:4, times = 5),
         possibility = rep(1:5, each = 4)) # this creates a new possibility column


d %>% 
  ggplot(aes(x = x, y = possibility, 
             fill = value %>% as.character())) +
  geom_point(shape = 21, size = 5) +
  scale_fill_manual(values = c("white", "navy")) +
  scale_x_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(.75, 4.25),
                  ylim = c(.75, 5.25)) +
  theme(legend.position = "none")
```



You can do the rest of the graph at your own leasure. I just wanted to show you how to make a "tibble." Later on, methods like this will be useful for creating simulated datasets. 







